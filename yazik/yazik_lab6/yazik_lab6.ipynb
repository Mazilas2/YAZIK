{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Практическая работа №6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание:\n",
    "Создать модель классификации текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение:\n",
    "1 В качестве исходного набора взять набор:\n",
    "Повышенный уровень (7 баллов): \n",
    "\n",
    "https://www.kaggle.com/datasets/prox37/sentiment-analysis-data На базовом уровне бинарная классификация, на повышенном многоклассовая. Набор данных необходимо предобработать (убрать лишнее, привести к нижнему регистру, лемматизировать и тд). Тексты подготовить к векторизации."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\artem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\artem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\artem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from tqdm import tqdm\n",
    "from pandarallel import pandarallel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
=======
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
>>>>>>> parent of 9a8903f (lab6)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"polarity\", \"id\", \"date\", \"query\", \"username\", \"text\"] \n",
    "df = pd.read_csv(\"./training.1600000.processed.noemoticon.csv\", delimiter=\",\", encoding=\"latin\", names=column_names)[:800000]\n",
    "df = df[[\"polarity\",\"text\"]]\n",
    "df['Text_Preprocess'] = ''\n",
    "df.to_csv(\"preprocess_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>text</th>\n",
       "      <th>Text_Preprocess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity                                               text Text_Preprocess\n",
       "0         0  @switchfoot http://twitpic.com/2y1zl - Awww, t...                \n",
       "1         0  is upset that he can't update his Facebook by ...                \n",
       "2         0  @Kenichan I dived many times for the ball. Man...                \n",
       "3         0    my whole body feels itchy and like its on fire                 \n",
       "4         0  @nationwideclass no, it's not behaving at all....                "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from string import punctuation\n",
    "\n",
    "def preprocess(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    # Lower case\n",
    "    sentences = [sentence.lower() for sentence in sentences]\n",
    "    # Remove punctuation\n",
    "    sentences = [''.join(c for c in sentence if c not in punctuation) for sentence in sentences]\n",
    "    # Tokenize words\n",
    "    sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "    # Remove stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    sentences = [[word for word in sentence if word not in stop_words] for sentence in sentences]\n",
    "    # Remove links \n",
    "    sentences = [[word for word in sentence if not word.startswith(\"http\")] for sentence in sentences]\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentences = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   polarity                                               text  \\\n",
      "0         0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
      "1         0  is upset that he can't update his Facebook by ...   \n",
      "2         0  @Kenichan I dived many times for the ball. Man...   \n",
      "3         0    my whole body feels itchy and like its on fire    \n",
      "4         0  @nationwideclass no, it's not behaving at all....   \n",
      "\n",
      "                                   Preprocessed_text  \n",
      "0  switchfoot awww thats bummer shoulda got david...  \n",
      "1  upset cant update facebook texting might cry r...  \n",
      "2  kenichan dived many time ball managed save 50 ...  \n",
      "3                    whole body feel itchy like fire  \n",
      "4          nationwideclass behaving im mad  cant see  \n"
     ]
    }
   ],
   "source": [
    "df_prep = pd.read_csv('./preprocess_data.csv')\n",
    "df_prep = df_prep.dropna(subset=['Preprocessed_text'])\n",
    "print(df_prep.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_prep['Preprocessed_text']\n",
    "y = df_prep['polarity']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=doc.split(), tags=[str(i)]) for i, doc in enumerate(X_train)]\n",
    "\n",
    "model = Doc2Vec(min_count=5, vector_size=100, window=5, workers=6, seed=42, epochs=50, batch_words=10000, compute_loss=True, hs=0, negative=10)\n",
    "model.build_vocab(tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 completed. Time elapsed: 98.32 seconds\n",
      "Epoch 2/50 completed. Time elapsed: 98.28 seconds\n",
      "Epoch 3/50 completed. Time elapsed: 100.89 seconds\n",
      "Epoch 4/50 completed. Time elapsed: 98.20 seconds\n",
      "Epoch 5/50 completed. Time elapsed: 98.13 seconds\n",
      "Epoch 6/50 completed. Time elapsed: 98.35 seconds\n",
      "Epoch 7/50 completed. Time elapsed: 97.80 seconds\n",
      "Epoch 8/50 completed. Time elapsed: 97.77 seconds\n",
      "Epoch 9/50 completed. Time elapsed: 97.73 seconds\n",
      "Epoch 10/50 completed. Time elapsed: 98.09 seconds\n",
      "Epoch 11/50 completed. Time elapsed: 97.90 seconds\n",
      "Epoch 12/50 completed. Time elapsed: 97.76 seconds\n",
      "Epoch 13/50 completed. Time elapsed: 97.88 seconds\n",
      "Epoch 14/50 completed. Time elapsed: 98.04 seconds\n",
      "Epoch 15/50 completed. Time elapsed: 97.91 seconds\n",
      "Epoch 16/50 completed. Time elapsed: 97.88 seconds\n",
      "Epoch 17/50 completed. Time elapsed: 97.98 seconds\n",
      "Epoch 18/50 completed. Time elapsed: 97.90 seconds\n",
      "Epoch 19/50 completed. Time elapsed: 97.96 seconds\n",
      "Epoch 20/50 completed. Time elapsed: 97.70 seconds\n",
      "Epoch 21/50 completed. Time elapsed: 97.98 seconds\n",
      "Epoch 22/50 completed. Time elapsed: 97.79 seconds\n",
      "Epoch 23/50 completed. Time elapsed: 98.00 seconds\n",
      "Epoch 24/50 completed. Time elapsed: 97.90 seconds\n",
      "Epoch 25/50 completed. Time elapsed: 97.93 seconds\n",
      "Epoch 26/50 completed. Time elapsed: 97.85 seconds\n",
      "Epoch 27/50 completed. Time elapsed: 97.94 seconds\n",
      "Epoch 28/50 completed. Time elapsed: 97.88 seconds\n",
      "Epoch 29/50 completed. Time elapsed: 98.30 seconds\n",
      "Epoch 30/50 completed. Time elapsed: 98.16 seconds\n",
      "Epoch 31/50 completed. Time elapsed: 97.85 seconds\n",
      "Epoch 32/50 completed. Time elapsed: 97.83 seconds\n",
      "Epoch 33/50 completed. Time elapsed: 98.09 seconds\n",
      "Epoch 34/50 completed. Time elapsed: 98.03 seconds\n",
      "Epoch 35/50 completed. Time elapsed: 97.95 seconds\n",
      "Epoch 36/50 completed. Time elapsed: 97.76 seconds\n",
      "Epoch 37/50 completed. Time elapsed: 97.75 seconds\n",
      "Epoch 38/50 completed. Time elapsed: 97.98 seconds\n",
      "Epoch 39/50 completed. Time elapsed: 97.90 seconds\n",
      "Epoch 40/50 completed. Time elapsed: 97.87 seconds\n",
      "Epoch 41/50 completed. Time elapsed: 98.09 seconds\n",
      "Epoch 42/50 completed. Time elapsed: 98.09 seconds\n",
      "Epoch 43/50 completed. Time elapsed: 98.18 seconds\n",
      "Epoch 44/50 completed. Time elapsed: 98.01 seconds\n",
      "Epoch 45/50 completed. Time elapsed: 98.27 seconds\n",
      "Epoch 46/50 completed. Time elapsed: 98.15 seconds\n",
      "Epoch 47/50 completed. Time elapsed: 98.08 seconds\n",
      "Epoch 48/50 completed. Time elapsed: 98.03 seconds\n",
      "Epoch 49/50 completed. Time elapsed: 101.17 seconds\n",
      "Epoch 50/50 completed. Time elapsed: 100.58 seconds\n",
      "Training complete. Model saved to \"doc2vec_model\"\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 50\n",
    "for epoch in range(total_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=1)\n",
    "    end_time = time.time()\n",
    "    time_elapsed = end_time - start_time\n",
    "    print(f'Epoch {epoch + 1}/{total_epochs} completed. Time elapsed: {time_elapsed:.2f} seconds')\n",
    "\n",
    "model.save('doc2vec_model.model')\n",
    "\n",
    "# Training is finished, and the model is saved\n",
    "print('Training complete. Model saved to \"doc2vec_model\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1279715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 146199/1279715 [00:20<02:36, 7225.74it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\Labs\\YAZIK\\YAZIK-1\\yazik\\yazik_lab6\\yazik_lab6.ipynb Cell 12\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Labs/YAZIK/YAZIK-1/yazik/yazik_lab6/yazik_lab6.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m X_train:\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Labs/YAZIK/YAZIK-1/yazik/yazik_lab6/yazik_lab6.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(text)  \u001b[39m# Ensure text is a string\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Labs/YAZIK/YAZIK-1/yazik/yazik_lab6/yazik_lab6.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     vector \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49minfer_vector(text\u001b[39m.\u001b[39;49msplit())\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Labs/YAZIK/YAZIK-1/yazik/yazik_lab6/yazik_lab6.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     inferred_vectors\u001b[39m.\u001b[39mappend(vector)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Labs/YAZIK/YAZIK-1/yazik/yazik_lab6/yazik_lab6.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     pbar\u001b[39m.\u001b[39mupdate(\u001b[39m1\u001b[39m)  \u001b[39m# Update the progress bar\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\artem\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\models\\doc2vec.py:651\u001b[0m, in \u001b[0;36mDoc2Vec.infer_vector\u001b[1;34m(self, doc_words, alpha, min_alpha, epochs)\u001b[0m\n\u001b[0;32m    646\u001b[0m         train_document_dm_concat(\n\u001b[0;32m    647\u001b[0m             \u001b[39mself\u001b[39m, doc_words, doctag_indexes, alpha, work, neu1,\n\u001b[0;32m    648\u001b[0m             learn_words\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, learn_hidden\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, doctag_vectors\u001b[39m=\u001b[39mdoctag_vectors, doctags_lockf\u001b[39m=\u001b[39mdoctags_lockf\n\u001b[0;32m    649\u001b[0m         )\n\u001b[0;32m    650\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 651\u001b[0m         train_document_dm(\n\u001b[0;32m    652\u001b[0m             \u001b[39mself\u001b[39;49m, doc_words, doctag_indexes, alpha, work, neu1,\n\u001b[0;32m    653\u001b[0m             learn_words\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, learn_hidden\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, doctag_vectors\u001b[39m=\u001b[39;49mdoctag_vectors, doctags_lockf\u001b[39m=\u001b[39;49mdoctags_lockf\n\u001b[0;32m    654\u001b[0m         )\n\u001b[0;32m    655\u001b[0m     alpha \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m alpha_delta\n\u001b[0;32m    657\u001b[0m \u001b[39mreturn\u001b[39;00m doctag_vectors[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the trained Doc2Vec model\n",
    "model = Doc2Vec.load('doc2vec_model.model')\n",
    "print(len(X_train))\n",
    "inferred_vectors = []\n",
    "\n",
    "with tqdm(total=len(X_train)) as pbar:\n",
    "    for text in X_train:\n",
    "        text = str(text)  # Ensure text is a string\n",
    "        vector = model.infer_vector(text.split())\n",
    "        inferred_vectors.append(vector)\n",
    "        pbar.update(1)  # Update the progress bar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
=======
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
>>>>>>> parent of 9a8903f (lab6)
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800000\n",
      "1000 / 800000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./preprocess_data.csv')\n",
    "print(len(df['text']))\n",
    "for i, review in enumerate(df['text']):\n",
    "    if i % 1000 == 0 and i > 0:\n",
    "        print(f\"{i} / {len(df['text'])}\")\n",
    "        df.to_csv(\"preprocess_data.csv\"index=False)\n",
    "        break\n",
    "    if (df['Text_Preprocess'].iloc[i] == ''):\n",
    "        preprocessed_review = preprocess(review)\n",
    "        df['Text_Preprocess'].iloc[i] = preprocessed_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>Unnamed: 0.1.1.1</th>\n",
       "      <th>Unnamed: 0.1.1.1.1</th>\n",
       "      <th>polarity</th>\n",
       "      <th>text</th>\n",
       "      <th>Text_Preprocess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>[['switchfoot', 'awww', 'thats', 'bummer'], ['...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>[['upset', 'cant', 'update', 'facebook', 'text...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>[['kenichan', 'dived', 'many', 'time', 'ball']...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[['whole', 'body', 'feel', 'itchy', 'like', 'f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>[['nationwideclass', 'behaving'], ['im', 'mad'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  Unnamed: 0.1.1.1  \\\n",
       "0           0             0               0                 0   \n",
       "1           1             1               1                 1   \n",
       "2           2             2               2                 2   \n",
       "3           3             3               3                 3   \n",
       "4           4             4               4                 4   \n",
       "\n",
       "   Unnamed: 0.1.1.1.1  polarity  \\\n",
       "0                   0         0   \n",
       "1                   1         0   \n",
       "2                   2         0   \n",
       "3                   3         0   \n",
       "4                   4         0   \n",
       "\n",
       "                                                text  \\\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1  is upset that he can't update his Facebook by ...   \n",
       "2  @Kenichan I dived many times for the ball. Man...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                     Text_Preprocess  \n",
       "0  [['switchfoot', 'awww', 'thats', 'bummer'], ['...  \n",
       "1  [['upset', 'cant', 'update', 'facebook', 'text...  \n",
       "2  [['kenichan', 'dived', 'many', 'time', 'ball']...  \n",
       "3  [['whole', 'body', 'feel', 'itchy', 'like', 'f...  \n",
       "4  [['nationwideclass', 'behaving'], ['im', 'mad'...  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
